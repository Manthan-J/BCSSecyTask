The maze.py file contains the environment description and q-learning . First we model the problem by creating the class Maze . It defines all the movement of harry and the death eater and also the starting position of harry , death eater and cup. The class MazeEnvironment converts the model into a gymnasium environment . Then q learning is done on the environment . The q table is a function of both the position of harry and the death eater. The agent receives -1 reward for every momvement or running into the wall. If death eater catches harry agent recieves -100 reward and if agent reaches cup agent recieves 100 reward. Also if the death eater is near harry agent receives -10 reward and also if harry is near cup it agent recieves 10 reward. Running the finalrun.py file runs the environment with updated q table along wiht pygame visualisation.
Also I first started coding in a jupyter notebook file , later I realised that pygame does not run in a jupyter notebook . So In a hurry I copied all the code snippets into a single .py file . You can run the .ipynb file , it has the same code but the visualisation is done using normal print statements instead of pygame
